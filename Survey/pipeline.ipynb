{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "931ebe31",
   "metadata": {},
   "source": [
    "# EU Taxonomy Ontology Generation Pipeline \n",
    "This Jupyter notebook is an automated ontology construction tool designed to encode the **EU taxonomy of sustainable activities** into a semantically structured, machine-readable format. The primary function of this tool is to process structured data from Excel spreadsheets, detailing economic activities and their sustainability criteria related to climate adaptation and mitigation, and convert this information into **RDF (Resource Description Framework) triples**. \n",
    "\n",
    "In addition to parsing the data, we leverage the **ChatGPT API** to intelligently extract nuanced information from free-text descriptions within the spreadsheets. This involves identifying and extracting references to external resources and any limitations that qualify an activity's sustainability credentials. These extracted elements are then meticulously added to the ontology, enriching the taxonomy with detailed context and constraints that define sustainable practices within each activity. **PLEASE NOTE THAT THIS SECTION OF THE PIPELINE WILL NOT NEED TO BE RUN BY YOU** as it takes more time to complete. It is included in this notebook for demonstration purposes - its limits are clearly marked, please do *NOT* run the cells within these limits.\n",
    "\n",
    "The resulting ontology not only categorizes activities within their respective sectors but also provides an intricate web of information that users can navigate to understand the sustainability landscape of the EU's economic activities. This notebook **streamlines the transformation from raw data to a comprehensive ontology**, ensuring a robust and navigable dataset for stakeholders engaged in sustainable finance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e64dce",
   "metadata": {},
   "source": [
    "## Install Necessary Libraries\n",
    "\n",
    "Please uncomment and run the appropriate lines below if one of the packages is not installed.\n",
    "\n",
    "`NOTE:` the *openai* library is **NOT** needed for the survey. You do **NOT** need to install it, it is only here for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02458cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pandas\n",
    "# ! pip install rdflib\n",
    "\n",
    "# YOU DO NOT NEED TO INSTALL THIS (IT IS HERE FOR DEMONSTRATION ONLY):\n",
    "# ! pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3288a6",
   "metadata": {},
   "source": [
    "## Dataset import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8800e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import pandas as pd\n",
    "import re\n",
    "import rdflib\n",
    "import ast\n",
    "\n",
    "# from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b6497e",
   "metadata": {},
   "source": [
    "Read the taxonomy xlsx, as downloaded by the official site: https://ec.europa.eu/sustainable-finance-taxonomy/assets/documents/taxonomy.xlsx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307e839b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read specific collumns as strings\n",
    "def read_excel_as_string(file_path, sheet_name):\n",
    "    return pd.read_excel(file_path, sheet_name=sheet_name, header=0, dtype={'Activity number': str})\n",
    "\n",
    "# Read the different excel sheets and keep them in separate dataframes\n",
    "df_adaptation = read_excel_as_string('assets/taxonomy.xlsx', \"Climate adaptation\")\n",
    "df_mitigation= read_excel_as_string('assets/taxonomy.xlsx', \"Climate mitigation\")\n",
    "\n",
    "df_adaptation.drop(columns=\"Unnamed: 12\", inplace=True)\n",
    "df_mitigation.drop(columns=\"Unnamed: 12\", inplace=True)\n",
    "\n",
    "# Path to the manually created ontology schema\n",
    "schema_ttl_file = 'assets/taxonomy_schema.ttl'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d3b803",
   "metadata": {},
   "source": [
    "Show first lines to understand the dataset structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75864f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adaptation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94415fe",
   "metadata": {},
   "source": [
    "***\n",
    "## Generate ontology based on the schema\n",
    "#### Define functions for generation of instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dfce28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate sub nodes for each activity (line)\n",
    "def generate_sub_nodes(activity_id, info_type, row):\n",
    "    ttl_data = \"\"\n",
    "    specific_info_id = f'{activity_id}_{info_type}'\n",
    "    activity_name = row[\"Activity\"].replace('\"', '\\\\\"') if pd.notna(row[\"Activity\"]) else \"Unnamed Activity\"\n",
    "\n",
    "    ttl_data += f'# node for \"{info_type.capitalize()}\" specific information\\n'\n",
    "    ttl_data += f'{specific_info_id}\\n'\n",
    "    ttl_data += f'    a sml:Climate{info_type.capitalize()}Info;\\n'\n",
    "    ttl_data += f'    core:prefLabel \"{activity_name} ({info_type.capitalize()} Information)\"@en;\\n'  \n",
    "\n",
    "    # Handling contribution type, replacing nan with an empty string\n",
    "    contribution_type = row.get(\"Contribution type\", \"\")\n",
    "    if pd.isna(contribution_type) or contribution_type == \"\":\n",
    "        contribution_type = \"\"\n",
    "    ttl_data += f'    sml:contributionType \"{contribution_type}\";\\n'\n",
    "\n",
    "    sub_info_fields = {\n",
    "        'SubstantialContributionCriteria': 'Substantial contribution criteria',\n",
    "        'DNSHonClimateMitigation': 'DNSH on Climate mitigation',\n",
    "        'DNSHonClimateAdaptation': 'DNSH on Climate adaptation',\n",
    "        'DNSHonWater': 'DNSH on Water',\n",
    "        'DNSHonCircularEconomy': 'DNSH on Circular economy',\n",
    "        'DNSHonPollutionPrevention': 'DNSH on Pollution prevention',\n",
    "        'DNSHonBiodiversity': 'DNSH on Biodiversity',\n",
    "        'Footnotes': 'Footnotes'\n",
    "    }\n",
    "\n",
    "    # Iterate and create nodes for all fields, even if empty\n",
    "    for sub_info, column_name in sub_info_fields.items():\n",
    "        # Special handling to exclude irrelevant fields\n",
    "        if (info_type == \"adaptation\" and sub_info == \"DNSHonClimateAdaptation\") or \\\n",
    "           (info_type == \"mitigation\" and sub_info == \"DNSHonClimateMitigation\"):\n",
    "            continue\n",
    "        \n",
    "        field_value = row.get(column_name, \"\")\n",
    "        sub_info_id = f'{specific_info_id}_{sub_info}'\n",
    "        ttl_data += f'    sml:has{sub_info} {sub_info_id};\\n'  # Link sub-info to the main info node\n",
    "    \n",
    "    # ttl_data += f'    core:prefLabel \"{activity_name} ({info_type.capitalize()})\"@en;\\n'  \n",
    "    ttl_data += f'    sml:isPartOf {activity_id}.\\n\\n'  # Linking back to the main activity\n",
    "    \n",
    "    # Iterate and create nodes for all fields, even if empty\n",
    "    for sub_info, column_name in sub_info_fields.items():\n",
    "        # Special handling to exclude irrelevant fields\n",
    "        if (info_type == \"adaptation\" and sub_info == \"DNSHonClimateAdaptation\") or \\\n",
    "           (info_type == \"mitigation\" and sub_info == \"DNSHonClimateMitigation\"):\n",
    "            continue\n",
    "        \n",
    "        field_value = row.get(column_name, \"\")\n",
    "        if pd.isna(field_value) or field_value == \"\":\n",
    "            field_value = \"\"  # Assign an empty string if the field is NaN or empty\n",
    "        sub_info_id = f'{specific_info_id}_{sub_info}'\n",
    "        readable_sub_info = sub_info.replace(\"DNSHon\", \"DNSH on \")\n",
    "        ttl_data += f'{sub_info_id} a sml:{sub_info};\\n'\n",
    "        ttl_data += f'    core:prefLabel \"{activity_name} ({info_type.capitalize()} - {readable_sub_info})\"@en;\\n'  \n",
    "        ttl_data += f'    sml:description \"\"\"{field_value}\"\"\"@en;\\n'\n",
    "        ttl_data += f'    sml:isPartOf {specific_info_id}.\\n\\n'\n",
    "\n",
    "    return ttl_data\n",
    "\n",
    "\n",
    "# Function to generate ontology instances\n",
    "def generate_ttl(df_adaptation, df_mitigation):\n",
    "    ttl_data = \"\"\n",
    "    activity_ids = set()\n",
    "\n",
    "    # Combine unique activity numbers from both dataframes\n",
    "    unique_activity_numbers = pd.concat([df_adaptation['Activity number'], df_mitigation['Activity number']]).unique()\n",
    "\n",
    "    # Process each unique activity number\n",
    "    for activity_number in unique_activity_numbers:\n",
    "        activity_id = f't:{activity_number}'  # Assuming activity_number is a unique identifier\n",
    "\n",
    "        # Generate sub-nodes for adaptation if present\n",
    "        if activity_number in df_adaptation['Activity number'].values:\n",
    "            row = df_adaptation[df_adaptation['Activity number'] == activity_number].iloc[0]\n",
    "            ttl_data += generate_sub_nodes(activity_id, \"adaptation\", row)\n",
    "        \n",
    "        # Generate sub-nodes for mitigation if present\n",
    "        if activity_number in df_mitigation['Activity number'].values:\n",
    "            row = df_mitigation[df_mitigation['Activity number'] == activity_number].iloc[0]\n",
    "            ttl_data += generate_sub_nodes(activity_id, \"mitigation\", row)\n",
    "\n",
    "        # Only add basic info if it hasn't been added before\n",
    "        if activity_id not in activity_ids:\n",
    "            # Assume the activity label is the same in both dataframes\n",
    "            activity_label = row[\"Activity\"].replace('\"', '\\\\\"') if pd.notna(row[\"Activity\"]) else \"Unnamed Activity\"\n",
    "            sector_number = str(activity_number).split(\".\")[0] if pd.notna(activity_number) else \"\"\n",
    "            # Check if NACE codes exist and are not NaN\n",
    "            nace_codes = row[\"NACE\"].split(\", \") if pd.notna(row[\"NACE\"]) and isinstance(row[\"NACE\"], str) else None\n",
    "\n",
    "            ttl_data += f'#------------------basic info of {activity_id}\\n'\n",
    "            ttl_data += f'{activity_id}\\n'\n",
    "            ttl_data += f'    a sml:Activity;\\n'\n",
    "            ttl_data += f'    sml:belongsToSector t:{sector_number};\\n'\n",
    "            ttl_data += f'    sml:hasNaceCode {\" ,\".join([\"nace:\" + n.strip() for n in nace_codes])};\\n' if nace_codes else ''\n",
    "            ttl_data += f'    core:definition  \"\"\"{row.get(\"Description\", \"\")}\"\"\"@en;\\n'\n",
    "            ttl_data += f'    core:prefLabel \"{activity_label}\"@en;\\n'\n",
    "\n",
    "            # Check if there is adaptation or mitigation information\n",
    "            has_adaptation = activity_number in df_adaptation['Activity number'].values\n",
    "            has_mitigation = activity_number in df_mitigation['Activity number'].values\n",
    "            parts = []\n",
    "            if has_adaptation:\n",
    "                parts.append(f'{activity_id}_adaptation')\n",
    "            if has_mitigation:\n",
    "                parts.append(f'{activity_id}_mitigation')\n",
    "\n",
    "            if parts:\n",
    "                ttl_data += f'    sml:hasPart {\", \".join(parts)};\\n'\n",
    "            ttl_data += f'    sml:activityNumber \"{activity_number}\"^^xsd:string.\\n\\n'\n",
    "\n",
    "            activity_ids.add(activity_id)\n",
    "\n",
    "    return ttl_data\n",
    "\n",
    "def add_sector_activity_relationships(g, df_adaptation, df_mitigation):\n",
    "    # Iterate over activities and add the inverse relationship to the graph\n",
    "    for df in [df_adaptation, df_mitigation]:\n",
    "        for index, row in df.iterrows():\n",
    "            activity_number = row[\"Activity number\"]\n",
    "            sector_number = str(activity_number).split(\".\")[0]\n",
    "            if pd.notna(activity_number) and pd.notna(sector_number):\n",
    "                activity_uri = rdflib.URIRef(f'https://ec.europa.eu/sustainable-finance-taxonomy/assets/taxonomy/{activity_number}')\n",
    "                sector_uri = rdflib.URIRef(f'https://ec.europa.eu/sustainable-finance-taxonomy/assets/taxonomy/{sector_number}')\n",
    "                \n",
    "                # Add the inverse relationship\n",
    "                g.add((sector_uri, SML.hasActivity, activity_uri))\n",
    "\n",
    "    return g\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cae67fd",
   "metadata": {},
   "source": [
    "#### Call the functions to create the instances and save them to 'new_instances.ttl'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547a8401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the TTL instances\n",
    "combined_ttl = generate_ttl(df_adaptation, df_mitigation)\n",
    "\n",
    "# Save to a TTL file\n",
    "ttl_filename = 'assets/new_instances.ttl'\n",
    "with open(ttl_filename, 'w') as file:\n",
    "    file.write(combined_ttl)\n",
    "\n",
    "print(f'TTL data has been written to {ttl_filename}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ce0415",
   "metadata": {},
   "source": [
    "#### Combine new instances with existing schema to create ontology and save to 'taxonomy.ttl'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7545a866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the TTL file with the instances you've created\n",
    "instances_ttl_file = 'assets/new_instances.ttl'\n",
    "\n",
    "# Path for the new TTL file that will contain both schema and instances\n",
    "combined_ttl_file = 'assets/taxonomy.ttl'\n",
    "\n",
    "# Read the existing schema TTL file\n",
    "with open(schema_ttl_file, 'r') as file:\n",
    "    schema_content = file.read()\n",
    "\n",
    "# Read the generated instances TTL file\n",
    "with open(instances_ttl_file, 'r') as file:\n",
    "    instances_content = file.read()\n",
    "\n",
    "# Combine the contents\n",
    "combined_content = schema_content + '\\n\\n' + instances_content\n",
    "\n",
    "# Write the combined content back to a new TTL file\n",
    "with open(combined_ttl_file, 'w') as file:\n",
    "    file.write(combined_content)\n",
    "\n",
    "# Add sector-activity relations\n",
    "g = rdflib.Graph()\n",
    "SML = rdflib.Namespace(\"https://w3id.org/def/smls-owl#\")\n",
    "g.bind(\"sml\", SML)\n",
    "\n",
    "# Parse the existing ontology\n",
    "g.parse(\"assets/taxonomy.ttl\", format=\"turtle\")\n",
    "\n",
    "# Add inverse relationships for sectors and activities\n",
    "g = add_sector_activity_relationships(g, df_adaptation, df_mitigation)\n",
    "\n",
    "# Save the updated graph\n",
    "g.serialize(\"assets/taxonomy.ttl\", format=\"turtle\")\n",
    "\n",
    "print(f'Combined TTL data has been written to {combined_ttl_file}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6215e107",
   "metadata": {},
   "source": [
    "***\n",
    "`PLEASE DO NOT RUN THIS BLOCK! - IT IS HERE FOR DEMONSTRATION PURPOSES, RUNNING IT WOULD TAKE ADDITIONAL INSTALLS!`\n",
    "## Automatic information extraction from free text using ChatGPT API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df734894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Key to ChatGPT\n",
    "client = OpenAI(api_key=\"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\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ccf777",
   "metadata": {},
   "source": [
    "#### Define system and user prompts to be used for each activity by the ChatGPT 4 LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51de1e2b",
   "metadata": {},
   "source": [
    "**System prompt:** \n",
    "\n",
    "*Assume you are an expert in ontology creation. Your task is to extract from the free text information for an ontology of the EU taxonomy for sustainability reporting, focused on the construction sector. Reporting on the sustainability of business activities will become mandatory for all (large) corporations in Europe, in compliance with the CSRD and the EU taxonomy of sustainable activities. Your task is to help us to understand the ontology and explain it in a list of imporant concepts.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1c1b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"Assume you are an expert in ontology creation. Your task is to extract from the free text information for an ontology of the EU taxonomy for sustainability reporting. Reporting on the sustainability of business activities will become mandatory for all (large) corporations in Europe, in compliance with the CSRD and the EU taxonomy of sustainable activities. Your task is to help us to understand the ontology and extract the external resources mentioned, as well as any limitations that you may find defined. You will put this information to a json with keys leading back to where you found the information. \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0015f7d4",
   "metadata": {},
   "source": [
    "**User prompt:** \n",
    "\n",
    "Please provide a JSON object with details about external resources and limitations for the given activity. The JSON object should only have four keys:\n",
    "\n",
    "1. \"activity_name\": Contain the name of the activity.\n",
    "2. \"node\": Include the activity number followed by a suffix indicating the type (e.g., \"_mitigation\" or \"_adaptation\") and the specific column where information was found. If data is from the \"Description\" field, use only the activity number.\n",
    "3. \"list_of_external_resources\": List all specifically named external resources mentioned, including URLs if available.\n",
    "4. \"has_limitations\": List any specific limitations or conditions that apply to the activity, focusing on those with clear specifications or numerical details.\n",
    "Exclude generic or non-specific sources like \"scientific peer-reviewed publications\" or \"open source.\" Numbers in parentheses should reference the corresponding segment in the \"Footnotes\" column and not be listed as external links.\n",
    "\n",
    "The JSON should be formatted for direct use with Python's json.loads function, without any additional text or commentary. Include known appendices or documents directly within the JSON. Here is the info from this row and an example template for your reference (replace $ with curly brackets):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831c7d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt_template = \"\"\" \n",
    "Please provide a JSON object with details about external resources and limitations for the given activity. The JSON object should only have four keys:\n",
    "\n",
    "1. \"activity_name\": Contain the name of the activity.\n",
    "2. \"node\": Include the activity number followed by a suffix indicating the type (e.g., \"_mitigation\" or \"_adaptation\") and the specific column where information was found. If data is from the \"Description\" field, use only the activity number.\n",
    "3. \"list_of_external_resources\": List all specifically named external resources mentioned, including URLs if available.\n",
    "4. \"has_limitations\": List any specific limitations or conditions that apply to the activity, focusing on those with clear specifications or numerical details.\n",
    "Exclude generic or non-specific sources like \"scientific peer-reviewed publications\" or \"open source.\" Numbers in parentheses should reference the corresponding segment in the \"Footnotes\" column and not be listed as external links.\n",
    "\n",
    "The JSON should be formatted for direct use with Python's json.loads function, without any additional text or commentary. Include known appendices or documents directly within the JSON. Here is the info from this row and an example template for your reference (replace $ with curly brackets):\n",
    "\n",
    "# ACTIVITY NAME:\n",
    "{Activity}\n",
    "\n",
    "# ACTIVITY NUMBER\n",
    "{Activity_number}\n",
    "\n",
    "# DESCRIPTION:\n",
    "{Description}\n",
    "\n",
    "# SECTOR:\n",
    "{Sector}\n",
    "\n",
    "# CONTRIBUTION TYPE:\n",
    "{Contribution_type}\n",
    "\n",
    "# SUBSTANTIAL CONTRIBUTION CRITERIA:\n",
    "{Criteria}\n",
    "\n",
    "# {DNSH_specific_field}\n",
    "{DNSH_specific_value}\n",
    "\n",
    "# DNSH on Water\n",
    "{DNSH_water}   \n",
    "\n",
    "# DNSH on Circular Economy\n",
    "{DNSH_circular}\n",
    "\n",
    "# DNSH on Pollution Prevention\n",
    "{DNSH_pollution}\n",
    "\n",
    "# DNSH on Biodiversity\n",
    "{DNSH_biodiversity}\n",
    "\n",
    "# FOOTNOTES\n",
    "{Footnotes}\n",
    "\n",
    "\n",
    "#EXAMPLE\n",
    "\n",
    "$\n",
    "  \"activity_name\": \"Construction of new buildings\",\n",
    "  \"{Activity_number}\": $\n",
    "    \"list_of_external_resources\": [\n",
    "    \"https://susproc.jrc.ec.europa.eu/product-bureau/product-groups/412/documents\",\n",
    "      \"https://ec.europa.eu/growth/content/eu-construction-and-demolition-waste-protocol-0_en\",\n",
    "      \"https://www.iso.org/standard/69370.html\",\n",
    "      \"Decision 2000/532/EC\",\n",
    "      \"EU Construction and Demolition Waste Management Protocol\",\n",
    "      \"ISO 20887\"\n",
    "    ],\n",
    "    \"has_limitations\": [\n",
    "    ['should be done like this', 'should comply to that']\n",
    "    ]\n",
    "  $,\n",
    "  \"{Activity_number}{type_suffix}_SubstantialContributionCriteria\": $\n",
    "    \"list_of_external_resources\": [\n",
    "    ],\n",
    "    \"has_limitations\": [\n",
    "\n",
    "    ]\n",
    "  $,\n",
    "  \"{Activity_number}{type_suffix}_{DNSH_specific_field}\": $\n",
    "    \"list_of_external_resources\": [\n",
    "\n",
    "    ],\n",
    "    \"has_limitations\": []\n",
    "  $,\n",
    "  \"{Activity_number}{type_suffix}_DNSHonWater\": $\n",
    "    \"list_of_external_resources\": [\n",
    "    ],\n",
    "    \"has_limitations\": []\n",
    "  $,\n",
    "  \"{Activity_number}{type_suffix}_DNSHonCircularEconomy\": $\n",
    "    \"list_of_external_resources\": [\n",
    "    \n",
    "    ],\n",
    "    \"has_limitations\": [\n",
    "    ]\n",
    "  $,\n",
    "  \"{Activity_number}{type_suffix}_DNSHonPollutionPrevention\": $\n",
    "    \"list_of_external_resources\": [\n",
    "     \n",
    "    ],\n",
    "    \"has_limitations\": [\n",
    "    ]\n",
    "  $,\n",
    "  \"{Activity_number}{type_suffix}_DNSHonBiodiversity\": $\n",
    "    \"list_of_external_resources\": [\n",
    "  \n",
    "    ],\n",
    "    \"has_limitations\": [\n",
    "    ]\n",
    "  $,\n",
    "  \"{Activity_number}{type_suffix}_Footnotes\": $\n",
    "    \"list_of_external_resources\": [\n",
    "      \"https://susproc.jrc.ec.europa.eu/product-bureau/product-groups/412/documents\",\n",
    "      \"https://ec.europa.eu/growth/content/eu-construction-and-demolition-waste-protocol-0_en\",\n",
    "      \"https://www.iso.org/standard/69370.html\",\n",
    "      \"Decision 2000/532/EC\",\n",
    "      \"EU Construction and Demolition Waste Management Protocol\",\n",
    "      \"ISO 20887\"\n",
    "    ],\n",
    "    \"has_limitations\": []\n",
    "  $\n",
    "$\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4438946",
   "metadata": {},
   "source": [
    "#### Define functions for extracting information using ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7230b5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove markdown from GPT response\n",
    "def remove_markdown(json_with_markdown):\n",
    "    # Use regular expression to remove Markdown formatting\n",
    "    plain_json = re.sub(r'```json|```', '', json_with_markdown, flags=re.DOTALL)\n",
    "    \n",
    "    # Remove leading/trailing whitespaces\n",
    "    plain_json = plain_json.strip()\n",
    "\n",
    "    return plain_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79c86c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that extracts information for one row\n",
    "def extract_information(text, data_type, model=\"gpt-4-1106-preview\"):\n",
    "     # Determine the appropriate DNSH field based on data type\n",
    "    DNSH_field_name, DNSH_field_value = (\"DNSHonClimateMitigation\", text.get(\"DNSH on Climate mitigation\", \"\")) \\\n",
    "        if data_type == \"adaptation\" else \\\n",
    "        (\"DNSHonClimateAdaptation\", text.get(\"DNSH on Climate adaptation\", \"\"))\n",
    "\n",
    "\n",
    "    # Format the user prompt with dynamic DNSH field and type suffix\n",
    "    user_prompt = user_prompt_template.format(\n",
    "        Activity=text[\"Activity\"],\n",
    "        Activity_number=text[\"Activity number\"],\n",
    "        Description=text[\"Description\"],\n",
    "        Sector=text[\"Sector\"],\n",
    "        Contribution_type=text[\"Contribution type\"],\n",
    "        Criteria=text[\"Substantial contribution criteria\"],\n",
    "        Footnotes=text[\"Footnotes\"],\n",
    "        DNSH_specific_field= DNSH_field_name,\n",
    "        DNSH_specific_value= DNSH_field_value,\n",
    "        DNSH_water=text[\"DNSH on Water\"],\n",
    "        DNSH_circular=text[\"DNSH on Circular economy\"],\n",
    "        DNSH_pollution=text[\"DNSH on Pollution prevention\"],\n",
    "        DNSH_biodiversity=text[\"DNSH on Biodiversity\"],\n",
    "        type_suffix=\"_adaptation\" if data_type == \"adaptation\" else \"_mitigation\"\n",
    "    )\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            { \n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_prompt\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "\n",
    "# Function to process DataFrame and extracts information for all activities of dataframe\n",
    "def process_dataframe(df, data_type, target_sector):\n",
    "    df_columns = [\"activity_name\", \"node\", \"list_of_external_resources\", \"has_limitations\"]\n",
    "    df_extracted = pd.DataFrame(columns=df_columns)\n",
    "    json_objects = []\n",
    "\n",
    "    # Filter the dataframe for the target sector\n",
    "    filtered_df = df[df['Sector'] == target_sector]\n",
    "\n",
    "    for index, row in filtered_df.iterrows():\n",
    "        try:\n",
    "            extracted_row_info = extract_information(row, data_type)\n",
    "            print(f\"Extracted info for row {index}:\", extracted_row_info)  # Debug print\n",
    "            \n",
    "            plain_json_row = remove_markdown(extracted_row_info)\n",
    "            print(f\"Plain JSON for row {index}:\", plain_json_row)  # Debug print\n",
    "            \n",
    "            json_dict = json.loads(plain_json_row)\n",
    "            json_objects.append(json_dict)\n",
    "\n",
    "            rows = []\n",
    "            for node, details in json_dict.items():\n",
    "                if node == \"activity_name\":\n",
    "                    activity_name = details\n",
    "                else:\n",
    "                    row = {\n",
    "                        \"activity_name\": activity_name,\n",
    "                        \"node\": node,\n",
    "                        \"list_of_external_resources\": details.get(\"list_of_external_resources\", []),\n",
    "                        \"has_limitations\": details.get(\"has_limitations\", [])\n",
    "                    }\n",
    "                    rows.append(row)\n",
    "            \n",
    "            df_extracted = pd.concat([df_extracted, pd.DataFrame(rows)], ignore_index=True)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing row {index}: {e}\\nRow content: {row}\")\n",
    "\n",
    "\n",
    "    return df_extracted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a22708",
   "metadata": {},
   "source": [
    "#### Extract information from a specific sector and save to csv\n",
    "Select sector from:\n",
    "- Forestry (4 activities)\n",
    "- Environmental protection and restoration activities (1 activity)\n",
    "- Manufacturing (17 activities)\n",
    "- Energy (31 activities)\n",
    "- Water supply, sewerage, waste management and remediation (12 activities)\n",
    "- Transport (17 activities)\n",
    "- Construction and real estate (7 activities)\n",
    "- Information and communication (3 activities)\n",
    "- Professional, scientific and technical activities (3 activities)\n",
    "- Financial and insurance activities (2 activities)\n",
    "- Education (1 activity)\n",
    "- Human health and social work activities (1 activity)\n",
    "- Arts, entertainment and recreation (3 activities)\n",
    "\n",
    "\n",
    "**!!! DONT ACTUALLY RUN THIS BLOCK BECAUSE IT TAKES AROUND 20 MINUTES!!!**\n",
    "\n",
    "The extracted_adaptation.csv and extracted_mitigation.csv are provided together with this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1ea38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the target sector\n",
    "target_sector = \"Construction and real estate\"\n",
    "\n",
    "# Use the function for adaptation and mitigation with sector filtering\n",
    "df_extracted_adaptation = process_dataframe(df_adaptation, \"adaptation\", target_sector)\n",
    "df_extracted_adaptation.to_csv(\"assets/extracted_adaptation_construction.csv\")\n",
    "print(\"Extracted info from adaptation!\")\n",
    "\n",
    "df_extracted_mitigation = process_dataframe(df_mitigation, \"mitigation\", target_sector)\n",
    "df_extracted_mitigation.to_csv(\"assets/extracted_mitigation_construction.csv\")\n",
    "print(\"Extracted info from mitigation!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ad54c6",
   "metadata": {},
   "source": [
    "`END OF THE BLOCK WHICH YOU DO NOT NEED TO RUN. PLEASE RUN EVERYTHING AFTER THIS.`\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d8fc47",
   "metadata": {},
   "source": [
    "#### Add extracted information back to the ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2456a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define namepsace\n",
    "SML = rdflib.Namespace(\"https://w3id.org/def/smls-owl#\")\n",
    "\n",
    "def add_properties_to_node(graph, node, external_resources, limitations):\n",
    "    node_uri = rdflib.URIRef(f\"https://ec.europa.eu/sustainable-finance-taxonomy/assets/taxonomy/{node}\")\n",
    "    ext_res_prop = SML.refersExternalResource\n",
    "    lim_prop = SML.hasLimitation\n",
    "\n",
    "    # Handle external resources\n",
    "    if external_resources:\n",
    "        # Safely evaluate string representation of a list, or split a string\n",
    "        resources = ast.literal_eval(external_resources) if external_resources.startswith(\"[\") else external_resources.split(\", \")\n",
    "        for res in resources:\n",
    "            graph.add((node_uri, ext_res_prop, rdflib.Literal(res.strip())))\n",
    "\n",
    "    # Handle limitations\n",
    "    if limitations:\n",
    "        # Safely evaluate string representation of a list, or split a string\n",
    "        limits = ast.literal_eval(limitations) if limitations.startswith(\"[\") else limitations.split(\", \")\n",
    "        for lim in limits:\n",
    "            graph.add((node_uri, lim_prop, rdflib.Literal(lim.strip())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42128e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add info to ttl file\n",
    "df_extracted_mitigation = pd.read_csv('assets/extracted_mitigation_construction.csv')\n",
    "df_extracted_adaptation = pd.read_csv('assets/extracted_adaptation_construction.csv')\n",
    "\n",
    "# Define namespaces and properties\n",
    "g = rdflib.Graph()\n",
    "g.bind(\"sml\", SML)\n",
    "\n",
    "# Parse the existing ontology\n",
    "g.parse(\"assets/taxonomy.ttl\", format=\"turtle\")\n",
    "\n",
    "# Update graph with mitigation data\n",
    "for index, row in df_extracted_mitigation.iterrows():\n",
    "    add_properties_to_node(g, row['node'], row['list_of_external_resources'], row['has_limitations'])\n",
    "\n",
    "# Update graph with adaptation data\n",
    "for index, row in df_extracted_adaptation.iterrows():\n",
    "    add_properties_to_node(g, row['node'], row['list_of_external_resources'], row['has_limitations'])\n",
    "\n",
    "# Save the updated graph\n",
    "g.serialize(\"final_taxonomy_with_7.ttl\", format=\"turtle\")\n",
    "\n",
    "print(\"Added info the the ontology.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
